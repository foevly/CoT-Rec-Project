{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6hy2qlhUvuw",
        "outputId": "9e35cce4-9748-4a3d-a72b-8d28c3b77490"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W58ru_tgQmN6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bec3beb-84e4-4e5e-95eb-78ddedce14ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import optim, nn\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "o0J8efYeSI5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ItemSequenceDataset:\n",
        "    def __init__(self, filepath, max_length):\n",
        "        import pandas as pd\n",
        "        df = pd.read_csv(filepath, names=['user_id', 'item_id'], usecols=[0, 1])\n",
        "        self.num_users = df['user_id'].max() + 1\n",
        "        self.num_items = df['item_id'].max() + 1\n",
        "\n",
        "        # 按用户分组\n",
        "        self.all_records = [[] for _ in range(self.num_users)]\n",
        "        for _, row in tqdm(df.iterrows(), desc=\"Loading data\"):\n",
        "            user_id, item_id = row.iloc[0], row.iloc[1]\n",
        "            self.all_records[user_id].append(item_id)\n",
        "\n",
        "        print(f'# Users: {self.num_users}')\n",
        "        print(f'# Items: {self.num_items}')\n",
        "        print(f'# Interactions: {len(df)}')\n",
        "\n",
        "        # 准备训练/验证/测试数据\n",
        "        X_train, y_train = [], []\n",
        "        X_valid, y_valid = [], []\n",
        "        X_test, y_test = [], []\n",
        "\n",
        "        for seq in tqdm(self.all_records, desc=\"Preparing sequences\"):\n",
        "            if len(seq) < 3:\n",
        "                continue\n",
        "\n",
        "            # 训练集: 使用前n-2个\n",
        "            train_seq = seq[:-2]\n",
        "            if len(train_seq) < max_length:\n",
        "                X_train.append([self.num_items] * (max_length - len(train_seq) + 1) + train_seq[:-1])\n",
        "                y_train.append([self.num_items] * (max_length - len(train_seq) + 1) + train_seq[1:])\n",
        "            else:\n",
        "                for i in range(len(train_seq) - max_length):\n",
        "                    X_train.append(train_seq[i:i+max_length])\n",
        "                    y_train.append(train_seq[i+1:i+max_length+1])\n",
        "\n",
        "            # 验证集: 使用前n-1个预测第n-1个\n",
        "            valid_seq = seq[:-1]\n",
        "            if len(valid_seq) - 1 < max_length:\n",
        "                X_valid.append([self.num_items] * (max_length - len(valid_seq) + 1) + valid_seq[:-1])\n",
        "            else:\n",
        "                X_valid.append(valid_seq[-(max_length+1):-1])\n",
        "            y_valid.append(valid_seq[-1])\n",
        "\n",
        "            # 测试集: 使用前n个预测第n个\n",
        "            test_seq = seq\n",
        "            if len(test_seq) - 1 < max_length:\n",
        "                X_test.append([self.num_items] * (max_length - len(test_seq) + 1) + test_seq[:-1])\n",
        "            else:\n",
        "                X_test.append(test_seq[-(max_length+1):-1])\n",
        "            y_test.append(test_seq[-1])\n",
        "\n",
        "        self.X_train = torch.tensor(X_train)\n",
        "        self.y_train = torch.tensor(y_train)\n",
        "        self.X_valid = torch.tensor(X_valid)\n",
        "        self.y_valid = torch.tensor(y_valid)\n",
        "        self.X_test = torch.tensor(X_test)\n",
        "        self.y_test = torch.tensor(y_test)\n",
        "\n",
        "        print('Data loading completed.')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_train)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X_train[idx], self.y_train[idx]"
      ],
      "metadata": {
        "id": "bUb2Xn1qSA7r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model & Metrics definition"
      ],
      "metadata": {
        "id": "LpD43j2FSMC1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e4a8b80"
      },
      "source": [
        "class UnidirectionalSelfAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "        self.key = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "        self.value = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len, embedding_dim = x.size(1), x.size(2)\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (embedding_dim ** 0.5)\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len)).bool().unsqueeze(0).to(x.device)\n",
        "        attention_scores = attention_scores.masked_fill(~mask, float('-inf'))\n",
        "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
        "        attention_output = torch.matmul(attention_weights, V)\n",
        "\n",
        "        return attention_output\n",
        "\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, embedding_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.attn = UnidirectionalSelfAttention(embedding_dim)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embedding_dim, embedding_dim)\n",
        "        )\n",
        "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output = self.attn(x)\n",
        "        x = self.layer_norm(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.layer_norm(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "\n",
        "class SASRec(nn.Module):\n",
        "    def __init__(self, num_items, embedding_dim=64, max_length=50, num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.item_embedding = nn.Embedding(num_items + 1, embedding_dim, padding_idx=num_items)\n",
        "        self.position_embedding = nn.Embedding(max_length, embedding_dim)\n",
        "        self.attn_layers = nn.ModuleList([\n",
        "            TransformerLayer(embedding_dim, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.normal_(module.weight, std=0.001)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                nn.init.normal_(module.weight, std=0.001)\n",
        "                if module.padding_idx is not None:\n",
        "                    module.weight.data[module.padding_idx].zero_()\n",
        "            elif isinstance(module, nn.LayerNorm):\n",
        "                nn.init.zeros_(module.bias)\n",
        "                nn.init.ones_(module.weight)\n",
        "\n",
        "    def forward(self, item_ids):\n",
        "        batch_size, seq_len = item_ids.shape\n",
        "        positions = torch.arange(seq_len, device=item_ids.device).unsqueeze(0).repeat(batch_size, 1)\n",
        "\n",
        "        item_embeds = self.item_embedding(item_ids)\n",
        "        pos_embeds = self.position_embedding(positions)\n",
        "        x = item_embeds + pos_embeds\n",
        "\n",
        "        for attn_layer in self.attn_layers:\n",
        "            x = attn_layer(x)\n",
        "\n",
        "        logits = torch.matmul(x, self.item_embedding.weight[:-1].T)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ==================== 评估工具 ====================\n",
        "class Metrics:\n",
        "    def __init__(self, topk_list):\n",
        "        self.topk_list = topk_list\n",
        "        self.hit_total = {k: 0 for k in topk_list}\n",
        "        self.ndcg_total = {k: 0 for k in topk_list}\n",
        "        self.mrr_total = {k: 0 for k in topk_list}\n",
        "        self.rec_list = []\n",
        "        self.total_nums = 0\n",
        "\n",
        "    def accumulate(self, ranks_list, y):\n",
        "        import math\n",
        "        for i, (ranks, true_item) in enumerate(zip(ranks_list, y)):\n",
        "            if true_item in ranks:\n",
        "                rank = ranks.index(true_item) + 1\n",
        "                self.rec_list.append((self.total_nums + i, ranks, true_item))\n",
        "                for k in self.topk_list:\n",
        "                    if rank <= k:\n",
        "                        self.hit_total[k] += 1\n",
        "                        self.ndcg_total[k] += 1 / math.log2(rank + 1)\n",
        "                        self.mrr_total[k] += 1 / rank\n",
        "        self.total_nums += len(y)\n",
        "\n",
        "    def get(self):\n",
        "        hit = {k: self.hit_total[k] / self.total_nums for k in self.topk_list}\n",
        "        ndcg = {k: self.ndcg_total[k] / self.total_nums for k in self.topk_list}\n",
        "        mrr = {k: self.mrr_total[k] / self.total_nums for k in self.topk_list}\n",
        "        return hit, ndcg, mrr, self.rec_list\n",
        "\n",
        "\n",
        "def get_top_k_recommendations(scores, all_records, k, phase):\n",
        "    delta = 2 if phase == 'valid' else 1\n",
        "    for idx, interacted_items in enumerate(all_records):\n",
        "        scores[idx, interacted_items[:-delta]] = -torch.inf\n",
        "    _, top_indices = torch.topk(scores, k, dim=1)\n",
        "    return top_indices"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and Test"
      ],
      "metadata": {
        "id": "iJThRTgvSafY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(dataloader, model, loss_func, optimizer, epoch, device,\n",
        "                    use_amp=False, accumulation_steps=1):\n",
        "    \"\"\"训练一个epoch（支持混合精度和梯度累积）\"\"\"\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    # 混合精度训练的scaler\n",
        "    scaler = GradScaler() if use_amp else None\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for batch_idx, (X, y) in enumerate(pbar):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # 混合精度前向传播\n",
        "        if use_amp:\n",
        "            with autocast(): # Removed 'cuda'\n",
        "                logits = model(X)\n",
        "                logits = logits.view(-1, logits.size(2))\n",
        "                y_flat = y.view(-1)\n",
        "                loss = loss_func(logits, y_flat) / accumulation_steps\n",
        "        else:\n",
        "            logits = model(X)\n",
        "            logits = logits.view(-1, logits.size(2))\n",
        "            y_flat = y.view(-1)\n",
        "            loss = loss_func(logits, y_flat) / accumulation_steps\n",
        "\n",
        "        train_loss += loss.item() * accumulation_steps\n",
        "\n",
        "        # 反向传播\n",
        "        if use_amp:\n",
        "            scaler.scale(loss).backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        # 梯度更新\n",
        "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == num_batches:\n",
        "            if use_amp:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 清理GPU缓存\n",
        "            if (batch_idx + 1) % (accumulation_steps * 10) == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        pbar.set_postfix({'loss': f'{train_loss/(batch_idx+1):.4f}'})\n",
        "\n",
        "    avg_loss = train_loss / num_batches\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "# 修改evaluate函数，分批处理\n",
        "def evaluate(dataset, model, device, batch_size, topk_list, phase):\n",
        "    \"\"\"在验证集或测试集上评估（减少内存使用）\"\"\"\n",
        "    X_all = dataset.X_valid if phase == 'valid' else dataset.X_test\n",
        "    y_all = dataset.y_valid if phase == 'valid' else dataset.y_test\n",
        "\n",
        "    model.eval()\n",
        "    metrics = Metrics(topk_list)\n",
        "\n",
        "    # 减小评估batch size\n",
        "    eval_batch_size = min(batch_size, 32)  # 最多32\n",
        "\n",
        "    with torch.no_grad():\n",
        "        start = 0\n",
        "        pbar = tqdm(total=len(y_all), desc=f\"Evaluating {phase}\")\n",
        "\n",
        "        while start < len(y_all):\n",
        "            end = min(start + eval_batch_size, len(y_all))\n",
        "            X = X_all[start:end].to(device)\n",
        "            y = y_all[start:end]\n",
        "\n",
        "            # Predict\n",
        "            scores = model(X)[:, -1, :]\n",
        "\n",
        "            # Get Top-K recommendations\n",
        "            ranks_list = get_top_k_recommendations(\n",
        "                scores, dataset.all_records[start:end], max(topk_list), phase\n",
        "            )\n",
        "\n",
        "            # Accumulate metrics\n",
        "            metrics.accumulate(ranks_list.tolist(), y.tolist())\n",
        "\n",
        "            # Clean up\n",
        "            del X, scores, ranks_list\n",
        "\n",
        "            start = end\n",
        "            pbar.update(end - start)\n",
        "\n",
        "            # Periodically clean up cache\n",
        "            if start % (eval_batch_size * 10) == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "    hit, ndcg, mrr, rec_list = metrics.get()\n",
        "\n",
        "    print(f'\\n[{phase.upper()}]')\n",
        "    print(f\"Hit@{topk_list}: {hit}\")\n",
        "    print(f\"NDCG@{topk_list}: {ndcg}\")\n",
        "    print(f\"MRR@{topk_list}: {mrr}\")\n",
        "\n",
        "    return hit, ndcg, mrr, rec_list\n",
        "\n",
        "\n",
        "# Modify train_model call\n",
        "def train_model(config, dataset, model, dataloader, optimizer, loss_func):\n",
        "    patience = config.NUM_PATIENCE\n",
        "    best_ndcg_valid = 0.0\n",
        "    best_epoch = 0\n",
        "    best_hit = None\n",
        "    best_ndcg = None\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Start training\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Initial evaluation\n",
        "    print(\"\\nInitial evaluation (random):\")\n",
        "    evaluate(dataset, model, config.DEVICE, config.BATCH_SIZE, config.TOPK_LIST, 'valid')\n",
        "    # evaluate(dataset, model, config.DEVICE, config.BATCH_SIZE, config.TOPK_LIST, 'test')\n",
        "\n",
        "    for epoch in range(1, config.NUM_EPOCHS + 1):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Epoch {epoch}/{config.NUM_EPOCHS}\")\n",
        "        print('='*60)\n",
        "\n",
        "        # Train\n",
        "        train_loss = train_one_epoch(\n",
        "            dataloader, model, loss_func, optimizer, epoch, config.DEVICE,\n",
        "            use_amp=config.USE_AMP,\n",
        "            accumulation_steps=config.ACCUMULATION_STEPS\n",
        "        )\n",
        "        print(f\"\\nTrain Loss: {train_loss:.4f}\")\n",
        "\n",
        "        # Evaluate\n",
        "        hit_valid, ndcg_valid, mrr_valid, rec_list_valid = evaluate(\n",
        "            dataset, model, config.DEVICE, config.BATCH_SIZE, config.TOPK_LIST, 'valid'\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Save best model\n",
        "        current_ndcg = ndcg_valid[max(config.TOPK_LIST)]\n",
        "        if current_ndcg >= best_ndcg_valid:\n",
        "            patience = config.NUM_PATIENCE\n",
        "            best_ndcg_valid = current_ndcg\n",
        "            best_epoch = epoch\n",
        "\n",
        "            # Save\n",
        "            model_path = os.path.join(config.CHECKPOINT_DIR, f'{config.DATASET}.pth')\n",
        "            torch.save(model.state_dict(), model_path)  # Save only weights to save space\n",
        "\n",
        "            with open(os.path.join(config.CHECKPOINT_DIR, f'{config.DATASET}_rec_list_valid.pkl'), 'wb') as f:\n",
        "                pickle.dump(rec_list_valid, f)\n",
        "            with open(os.path.join(config.CHECKPOINT_DIR, f'{config.DATASET}_rec_list_test.pkl'), 'wb') as f:\n",
        "                pickle.dump(rec_list_test, f)\n",
        "\n",
        "            print(f\"\\n✅ Best model saved! NDCG@{max(config.TOPK_LIST)}: {best_ndcg_valid:.4f}\")\n",
        "\n",
        "            log_path = os.path.join(config.CHECKPOINT_DIR, f'{config.DATASET}.log')\n",
        "            with open(log_path, 'w') as f:\n",
        "                f.write(f'Best epoch: {best_epoch}\\n')\n",
        "                f.write(f'Valid - Hit: {hit_valid}, NDCG: {ndcg_valid}, MRR: {mrr_valid}\\n')\n",
        "                f.write(f'Test - Hit: {hit_test}, NDCG: {ndcg_test}, MRR: {mrr_test}\\n')\n",
        "        else:\n",
        "            patience -= 1\n",
        "            print(f\"\\nPatience: {patience}/{config.NUM_PATIENCE}\")\n",
        "\n",
        "            if patience == 0:\n",
        "                print(\"\\n⚠️ Early stopping!\")\n",
        "                break\n",
        "\n",
        "        # Clean up after each epoch\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Training finished!\")\n",
        "    print(f\"Best epoch: {best_epoch}\")\n",
        "    print(f\"Best NDCG@{max(config.TOPK_LIST)}: {best_ndcg_valid:.4f}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return best_epoch, best_ndcg_valid"
      ],
      "metadata": {
        "id": "P1rc4OtlSc_J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Config"
      ],
      "metadata": {
        "id": "vz1k1ZG5TKAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    # dataset name\n",
        "    DATASET = 'Grocery_and_Gourmet_Food'\n",
        "    # DATA_PATH = f'../data/{DATASET}.csv'\n",
        "    DATA_PATH = f'/content/drive/MyDrive/11785IDL/IDL_Project/data/{DATASET}.csv' # Updated path\n",
        "\n",
        "    # model config\n",
        "    EMBEDDING_DIM = 128\n",
        "    MAX_LENGTH = 32\n",
        "    NUM_LAYERS = 2\n",
        "    DROPOUT = 0.2\n",
        "\n",
        "    # train config\n",
        "    BATCH_SIZE = 64\n",
        "    NUM_EPOCHS = 10 # Increased epochs\n",
        "    NUM_PATIENCE = 5\n",
        "    LR = 0.001\n",
        "    TOPK_LIST = [10]\n",
        "\n",
        "    USE_AMP = True               # 混合精度训练\n",
        "    ACCUMULATION_STEPS = 4\n",
        "\n",
        "    # device and seed\n",
        "    DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    SEED = 2025\n",
        "\n",
        "    # checkpoint directory\n",
        "    CHECKPOINT_DIR = '/content/drive/MyDrive/11785IDL/IDL_Project/midterm_Baseline/checkpoint_baseline'"
      ],
      "metadata": {
        "id": "WyVm1GytTKwD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data"
      ],
      "metadata": {
        "id": "GBnB0x0xT9YC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(config):\n",
        "    \"\"\"load data\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"loading data\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"from: {config.DATA_PATH}\")\n",
        "\n",
        "    dataset = ItemSequenceDataset(config.DATA_PATH, config.MAX_LENGTH)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    print(f\"training batch: {len(dataloader)}\")\n",
        "\n",
        "    return dataset, dataloader"
      ],
      "metadata": {
        "id": "QzgJ0_iwT_vg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create model"
      ],
      "metadata": {
        "id": "1JKawfhNTL0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(config, num_items):\n",
        "    \"\"\"SASRec model\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Create model\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    model = SASRec(\n",
        "        num_items=num_items,\n",
        "        embedding_dim=config.EMBEDDING_DIM,\n",
        "        max_length=config.MAX_LENGTH,\n",
        "        num_layers=config.NUM_LAYERS,\n",
        "        dropout=config.DROPOUT\n",
        "    ).to(config.DEVICE)\n",
        "\n",
        "    # 优化器和损失函数\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.LR)\n",
        "    loss_func = nn.CrossEntropyLoss(ignore_index=num_items)\n",
        "\n",
        "    # Inspect model architecture and check to verify number of parameters of your network\n",
        "    try:\n",
        "        # Install and import torchsummaryX\n",
        "        !pip install torchsummaryX==1.1.0\n",
        "        from torchsummaryX import summary\n",
        "\n",
        "        # Create a dummy input tensor\n",
        "        dummy_input = torch.randint(0, num_items, (config.BATCH_SIZE, config.MAX_LENGTH), device=config.DEVICE)\n",
        "        summary(model, dummy_input)\n",
        "\n",
        "    except:\n",
        "        !pip install torchsummary\n",
        "        from torchsummary import summary\n",
        "\n",
        "        # Create a dummy input tensor and get its shape as a tuple\n",
        "        dummy_input = torch.randint(0, num_items, (config.BATCH_SIZE, config.MAX_LENGTH), dtype=torch.long).to(config.DEVICE)\n",
        "        summary(model, input_size=tuple(dummy_input.shape))\n",
        "\n",
        "\n",
        "    return model, optimizer, loss_func"
      ],
      "metadata": {
        "id": "GKBmSi4iUDSm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model"
      ],
      "metadata": {
        "id": "9iJE-gNCUGDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training(config):\n",
        "    \"\"\"运行完整的训练流程\"\"\"\n",
        "    # 设置随机种子\n",
        "    torch.manual_seed(config.SEED)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(config.SEED)\n",
        "\n",
        "    # 创建输出目录\n",
        "    os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "    # 1. 加载数据\n",
        "    dataset, dataloader = load_data(config)\n",
        "\n",
        "    # 2. 创建模型\n",
        "    model, optimizer, loss_func = create_model(config, dataset.num_items)\n",
        "\n",
        "    # 3. 训练模型\n",
        "    best_epoch, best_ndcg = train_model(\n",
        "        config, dataset, model, dataloader, optimizer, loss_func\n",
        "    )\n",
        "\n",
        "    # 4. 训练完成后加载最佳模型\n",
        "    model_path = os.path.join(config.CHECKPOINT_DIR, f'{config.DATASET}.pth')\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    # 5. 在测试集上评估（只评估一次）\n",
        "    hit_test, ndcg_test, mrr_test, rec_list_test = evaluate(\n",
        "        dataset, model, config.DEVICE, config.BATCH_SIZE, config.TOPK_LIST, 'test'\n",
        "    )\n",
        "\n",
        "    # 保存测试集推荐列表\n",
        "    with open(os.path.join(config.CHECKPOINT_DIR, f'{config.DATASET}_rec_list_test.pkl'), 'wb') as f:\n",
        "        pickle.dump(rec_list_test, f)\n",
        "\n",
        "    # 保存最终结果日志（简洁版）\n",
        "    final_log_path = os.path.join(config.CHECKPOINT_DIR, f'{config.DATASET}_final_results.log')\n",
        "    with open(final_log_path, 'w') as f:\n",
        "        f.write(f'Best epoch: {best_epoch}\\n')\n",
        "        f.write(f'Validation NDCG@10: {best_ndcg:.4f}\\n')\n",
        "        f.write(f'Test - Hit: {hit_test}, NDCG: {ndcg_test}, MRR: {mrr_test}\\n')\n",
        "\n",
        "    return best_epoch, best_ndcg, ndcg_test[max(config.TOPK_LIST)]"
      ],
      "metadata": {
        "id": "IcrGoOmnUG6u"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main"
      ],
      "metadata": {
        "id": "ikimtXg_UJLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    config = Config()\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"SASRec Baseline Training\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Device: {config.DEVICE}\")\n",
        "    print(f\"Dataset: {config.DATASET}\")\n",
        "    print(f\"Embedding dim: {config.EMBEDDING_DIM}\")\n",
        "    print(f\"Max length: {config.MAX_LENGTH}\")\n",
        "    print(f\"Transformer layers: {config.NUM_LAYERS}\")\n",
        "    print(f\"Batch size: {config.BATCH_SIZE}\")\n",
        "    print(f\"Num of epochs: {config.NUM_EPOCHS}\")\n",
        "    print(f\"lr: {config.LR}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 运行训练\n",
        "    best_epoch, best_ndcg = run_training(config)\n",
        "    # best_epoch, best_val_ndcg, final_test_ndcg = run_training(config)\n",
        "\n",
        "    print(f\"\\n Training finished\")\n",
        "    print(f\"Best result: Epoch {best_epoch}, NDCG@10 = {best_ndcg:.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "ZP8qLqNKUJ8l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73e81d52-c61f-4a8a-f7c5-607660cc1462"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "SASRec Baseline Training\n",
            "============================================================\n",
            "Device: cuda:0\n",
            "Dataset: Grocery_and_Gourmet_Food\n",
            "Embedding dim: 128\n",
            "Max length: 32\n",
            "Transformer layers: 2\n",
            "Batch size: 64\n",
            "Num od epochs: 10\n",
            "lr: 0.001\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "loading data\n",
            "============================================================\n",
            "from: /content/drive/MyDrive/11785IDL/IDL_Project/data/Grocery_and_Gourmet_Food.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading data: 4125640it [02:20, 29392.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Users: 419876\n",
            "# Items: 135194\n",
            "# Interactions: 4125640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preparing sequences: 100%|██████████| 419876/419876 [00:04<00:00, 103371.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loading completed.\n",
            "training batch: 10861\n",
            "\n",
            "============================================================\n",
            "Create model\n",
            "============================================================\n",
            "Requirement already satisfied: torchsummaryX==1.1.0 in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Layer                   Kernel Shape         Output Shape         # Params (K)      # Mult-Adds (M)\n",
            "====================================================================================================\n",
            "0_Embedding            [128, 135195]        [64, 32, 128]            17,304.96                17.30\n",
            "1_Embedding                [128, 32]        [64, 32, 128]                 4.10                 0.00\n",
            "2_Linear                  [128, 128]        [64, 32, 128]                16.38                 0.02\n",
            "3_Linear                  [128, 128]        [64, 32, 128]                16.38                 0.02\n",
            "4_Linear                  [128, 128]        [64, 32, 128]                16.38                 0.02\n",
            "5_Dropout                          -        [64, 32, 128]                    -                    -\n",
            "6_LayerNorm                    [128]        [64, 32, 128]                 0.26                 0.00\n",
            "7_Linear                  [128, 128]        [64, 32, 128]                16.51                 0.02\n",
            "8_ReLU                             -        [64, 32, 128]                    -                    -\n",
            "9_Linear                  [128, 128]        [64, 32, 128]                16.51                 0.02\n",
            "10_Dropout                         -        [64, 32, 128]                    -                    -\n",
            "11_LayerNorm                   [128]        [64, 32, 128]          (recursive)                 0.00\n",
            "12_Linear                 [128, 128]        [64, 32, 128]                16.38                 0.02\n",
            "13_Linear                 [128, 128]        [64, 32, 128]                16.38                 0.02\n",
            "14_Linear                 [128, 128]        [64, 32, 128]                16.38                 0.02\n",
            "15_Dropout                         -        [64, 32, 128]                    -                    -\n",
            "16_LayerNorm                   [128]        [64, 32, 128]                 0.26                 0.00\n",
            "17_Linear                 [128, 128]        [64, 32, 128]                16.51                 0.02\n",
            "18_ReLU                            -        [64, 32, 128]                    -                    -\n",
            "19_Linear                 [128, 128]        [64, 32, 128]                16.51                 0.02\n",
            "20_Dropout                         -        [64, 32, 128]                    -                    -\n",
            "21_LayerNorm                   [128]        [64, 32, 128]          (recursive)                 0.00\n",
            "====================================================================================================\n",
            "# Params:    17,473.92K\n",
            "# Mult-Adds: 17.47M\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "Start training\n",
            "============================================================\n",
            "\n",
            "Initial evaluation (random):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating valid:   0%|          | 0/419876 [02:59<?, ?it/s]\n",
            "/tmp/ipython-input-367210020.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler() if use_amp else None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[VALID]\n",
            "Hit@[10]: {10: 0.00011193781021063362}\n",
            "NDCG@[10]: {10: 4.7811128264238586e-05}\n",
            "MRR@[10]: {10: 2.8853000742320437e-05}\n",
            "\n",
            "============================================================\n",
            "Epoch 1/10\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:   0%|          | 0/10861 [00:00<?, ?it/s]/tmp/ipython-input-367210020.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(): # Removed 'cuda'\n",
            "Epoch 1: 100%|██████████| 10861/10861 [10:42<00:00, 16.89it/s, loss=6.6641]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 6.6641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating valid:   0%|          | 0/419876 [02:56<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[VALID]\n",
            "Hit@[10]: {10: 0.019441454143604302}\n",
            "NDCG@[10]: {10: 0.010365167522818266}\n",
            "MRR@[10]: {10: 0.007625480735279878}\n",
            "\n",
            "✅ Best model saved! NDCG@10: 0.0104\n",
            "\n",
            "============================================================\n",
            "Epoch 2/10\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 10861/10861 [10:42<00:00, 16.92it/s, loss=3.9615]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 3.9615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating valid:   0%|          | 0/419876 [02:56<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[VALID]\n",
            "Hit@[10]: {10: 0.02129438215092075}\n",
            "NDCG@[10]: {10: 0.01141163174315935}\n",
            "MRR@[10]: {10: 0.008429629715318868}\n",
            "\n",
            "✅ Best model saved! NDCG@10: 0.0114\n",
            "\n",
            "============================================================\n",
            "Epoch 3/10\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 10861/10861 [10:40<00:00, 16.96it/s, loss=3.3455]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 3.3455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating valid:   0%|          | 0/419876 [02:56<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[VALID]\n",
            "Hit@[10]: {10: 0.02171117186979013}\n",
            "NDCG@[10]: {10: 0.011675618737488251}\n",
            "MRR@[10]: {10: 0.00864706825008208}\n",
            "\n",
            "✅ Best model saved! NDCG@10: 0.0117\n",
            "\n",
            "============================================================\n",
            "Epoch 4/10\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 10861/10861 [10:39<00:00, 16.98it/s, loss=3.0791]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 3.0791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating valid:   0%|          | 0/419876 [02:55<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[VALID]\n",
            "Hit@[10]: {10: 0.022606674351475196}\n",
            "NDCG@[10]: {10: 0.012152880752748054}\n",
            "MRR@[10]: {10: 0.008989921741067386}\n",
            "\n",
            "✅ Best model saved! NDCG@10: 0.0122\n",
            "\n",
            "============================================================\n",
            "Epoch 5/10\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 10861/10861 [10:39<00:00, 16.98it/s, loss=2.9188]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 2.9188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating valid:   0%|          | 0/419876 [02:56<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[VALID]\n",
            "Hit@[10]: {10: 0.022280387542988882}\n",
            "NDCG@[10]: {10: 0.012117995227981078}\n",
            "MRR@[10]: {10: 0.009042891839419965}\n",
            "\n",
            "Patience: 4/5\n",
            "\n",
            "============================================================\n",
            "Epoch 6/10\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 10861/10861 [10:40<00:00, 16.95it/s, loss=2.8093]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 2.8093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating valid:   0%|          | 0/419876 [02:56<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[VALID]\n",
            "Hit@[10]: {10: 0.022775771894559347}\n",
            "NDCG@[10]: {10: 0.01238545859203993}\n",
            "MRR@[10]: {10: 0.009238691332452455}\n",
            "\n",
            "✅ Best model saved! NDCG@10: 0.0124\n",
            "\n",
            "============================================================\n",
            "Epoch 7/10\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 10861/10861 [10:39<00:00, 16.99it/s, loss=2.7255]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 2.7255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating valid:   0%|          | 0/419876 [02:55<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[VALID]\n",
            "Hit@[10]: {10: 0.02244472177500024}\n",
            "NDCG@[10]: {10: 0.01221069518853322}\n",
            "MRR@[10]: {10: 0.009112533526527232}\n",
            "\n",
            "Patience: 4/5\n",
            "\n",
            "============================================================\n",
            "Epoch 8/10\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 10861/10861 [10:39<00:00, 16.98it/s, loss=2.6578]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 2.6578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating valid:   0%|          | 0/419876 [02:55<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[VALID]\n",
            "Hit@[10]: {10: 0.02222322781011537}\n",
            "NDCG@[10]: {10: 0.012193201764138805}\n",
            "MRR@[10]: {10: 0.00915698826123577}\n",
            "\n",
            "Patience: 3/5\n",
            "\n",
            "============================================================\n",
            "Epoch 9/10\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 10861/10861 [10:38<00:00, 17.00it/s, loss=2.6045]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 2.6045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating valid:   0%|          | 0/419876 [02:55<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[VALID]\n",
            "Hit@[10]: {10: 0.022613819318084386}\n",
            "NDCG@[10]: {10: 0.012362767831103514}\n",
            "MRR@[10]: {10: 0.00926187183457193}\n",
            "\n",
            "Patience: 2/5\n",
            "\n",
            "============================================================\n",
            "Epoch 10/10\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 10861/10861 [10:39<00:00, 16.98it/s, loss=2.5569]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 2.5569\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating valid:   0%|          | 0/419876 [02:55<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[VALID]\n",
            "Hit@[10]: {10: 0.022549514618601683}\n",
            "NDCG@[10]: {10: 0.012379214530199393}\n",
            "MRR@[10]: {10: 0.009301326982856822}\n",
            "\n",
            "Patience: 1/5\n",
            "\n",
            "============================================================\n",
            "Training finished!\n",
            "Best epoch: 6\n",
            "Best NDCG@10: 0.0124\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating test:   0%|          | 0/419876 [02:55<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TEST]\n",
            "Hit@[10]: {10: 0.018900818336842307}\n",
            "NDCG@[10]: {10: 0.010185136535850757}\n",
            "MRR@[10]: {10: 0.007557184872570897}\n",
            "\n",
            " Training finished\n",
            "Best result: Epoch 6, Val_NDCG@10 = 0.0124, Test_NDCG@10: 0.0102\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}