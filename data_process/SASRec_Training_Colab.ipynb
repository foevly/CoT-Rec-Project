{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQgx9hKMR8e7"
   },
   "source": [
    "# SASRec Training for CoT-Rec\n",
    "\n",
    "This notebook trains the SASRec (Self-Attentive Sequential Recommendation) model, which serves as the retriever in Stage 1 of CoT-Rec.\n",
    "\n",
    "## Prerequisites\n",
    "1. Run `preprocess_amazon.py` to generate:\n",
    "   - `datasets/processed/Grocery_and_Gourmet_Food.csv`\n",
    "   - `datasets/processed/Grocery_and_Gourmet_Food.json`\n",
    "2. Upload these files to Colab or mount Google Drive\n",
    "\n",
    "## Output\n",
    "After training, this notebook will generate:\n",
    "- `SASRec/checkpoint/Grocery_and_Gourmet_Food.pth` - Trained model\n",
    "- `SASRec/checkpoint/Grocery_and_Gourmet_Food_rec_list_valid.pkl` - Validation recommendations\n",
    "- `SASRec/checkpoint/Grocery_and_Gourmet_Food_rec_list_test.pkl` - Test recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSYOCOlvR8e-"
   },
   "source": [
    "## Step 0: Setup and Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 12446,
     "status": "ok",
     "timestamp": 1763759895630,
     "user": {
      "displayName": "Tom Twan",
      "userId": "01823172147373690392"
     },
     "user_tz": 300
    },
    "id": "Ois1ZqVvR8e-"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch pandas tqdm -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "wvyMiQypR8e_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive (if files are stored there)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Or upload files directly in Colab\n",
    "# Set your working directory\n",
    "import os\n",
    "WORK_DIR = '/content/drive/MyDrive/CoT-Rec'  # Change this to your directory\n",
    "os.chdir(WORK_DIR)\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs('SASRec/checkpoint', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTtIpUIVR8e_"
   },
   "source": [
    "## Step 1: Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XskYT96YR8e_"
   },
   "outputs": [],
   "source": [
    "# Configuration - Modify these as needed\n",
    "DATASET_NAME = 'Grocery_and_Gourmet_Food'\n",
    "EMBEDDING_DIM = 128\n",
    "MAX_LENGTH = 32\n",
    "NUM_LAYERS = 2\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 100\n",
    "NUM_PATIENCE = 5  # Early stopping patience\n",
    "LEARNING_RATE = 1e-3\n",
    "DROPOUT = 0.2\n",
    "TOPK_LIST = [10]  # Top-k for evaluation\n",
    "VERBOSE = 100  # Print loss every N batches\n",
    "SEED = 2025\n",
    "\n",
    "# Device configuration\n",
    "import torch\n",
    "DEVICE_ID = 0  # Use GPU 0, set to -1 for CPU\n",
    "device = torch.device(f'cuda:{DEVICE_ID}' if DEVICE_ID >= 0 and torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# File paths\n",
    "FILEPATH = f'datasets/processed/{DATASET_NAME}.csv'\n",
    "print(f\"Data file: {FILEPATH}\")\n",
    "print(f\"Checkpoint directory: SASRec/checkpoint/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5t28oIfYR8fA"
   },
   "source": "## Step 2: Data Loading Class\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4XRIs6QR8fA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ItemSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for sequential recommendation.\n",
    "    Splits each user's sequence into train/valid/test sets.\n",
    "    \"\"\"\n",
    "    def __init__(self, filepath, max_length):\n",
    "        df = pd.read_csv(filepath, names=['user_id', 'item_id'], usecols=[0, 1])\n",
    "        self.num_users, self.num_items = df['user_id'].max() + 1, df['item_id'].max() + 1\n",
    "\n",
    "        # Build sequences for each user\n",
    "        self.all_records = [[] for _ in range(self.num_users)]\n",
    "        for _, row in tqdm(df.iterrows(), desc=\"Building user sequences\"):\n",
    "            user_id, item_id = row.iloc[0], row.iloc[1]\n",
    "            self.all_records[user_id].append(item_id)\n",
    "\n",
    "        print('# Users:', self.num_users)\n",
    "        print('# Items:', self.num_items)\n",
    "        print('# Interactions:', len(df))\n",
    "\n",
    "        X_train, y_train = [], []\n",
    "        X_valid, y_valid = [], []\n",
    "        X_test, y_test = [], []\n",
    "\n",
    "        # Split sequences into train/valid/test\n",
    "        for seq in tqdm(self.all_records, desc=\"Creating sequences\"):\n",
    "            # Training: all except last 2 items\n",
    "            train_seq = seq[:-2]\n",
    "            if len(train_seq) < max_length:\n",
    "                X_train.append((max_length - len(train_seq) + 1) * [self.num_items] + train_seq[:-1])\n",
    "                y_train.append((max_length - len(train_seq) + 1) * [self.num_items] + train_seq[1:])\n",
    "            else:\n",
    "                for i in range(len(train_seq) - max_length):\n",
    "                    X_train.append(train_seq[i:i+max_length])\n",
    "                    y_train.append(train_seq[i+1:i+max_length+1])\n",
    "\n",
    "            # Validation: all except last 1 item (predicts second-to-last)\n",
    "            valid_seq = seq[:-1]\n",
    "            if len(valid_seq) - 1 < max_length:\n",
    "                X_valid.append((max_length - len(valid_seq) + 1) * [self.num_items] + valid_seq[:-1])\n",
    "            else:\n",
    "                X_valid.append(valid_seq[-(max_length+1):-1])\n",
    "            y_valid.append(valid_seq[-1])\n",
    "\n",
    "            # Test: full sequence (predicts last item)\n",
    "            test_seq = seq\n",
    "            if len(test_seq) - 1 < max_length:\n",
    "                X_test.append((max_length - len(test_seq) + 1) * [self.num_items] + test_seq[:-1])\n",
    "            else:\n",
    "                X_test.append(test_seq[-(max_length+1):-1])\n",
    "            y_test.append(test_seq[-1])\n",
    "\n",
    "        self.X_train, self.y_train = torch.tensor(X_train), torch.tensor(y_train)\n",
    "        self.X_valid, self.y_valid = torch.tensor(X_valid), torch.tensor(y_valid)\n",
    "        self.X_test, self.y_test = torch.tensor(X_test), torch.tensor(y_test)\n",
    "\n",
    "        print('Data loading completed.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_train[idx], self.y_train[idx]\n",
    "\n",
    "print(\"‚úÖ ItemSequenceDataset class loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7rCKp8nR8fA"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRjXugs-R8fB"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class UnidirectionalSelfAttention(nn.Module):\n",
    "    \"\"\"Unidirectional (causal) self-attention layer.\"\"\"\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(UnidirectionalSelfAttention, self).__init__()\n",
    "        self.query = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "        self.key = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "        self.value = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batch_size, seq_len, embedding_dim]\n",
    "        seq_len, embedding_dim = x.size(1), x.size(2)\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (embedding_dim ** 0.5)\n",
    "        # Causal mask: only attend to previous positions\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len)).bool().unsqueeze(0).to(attention_scores.device)\n",
    "        attention_scores = attention_scores.masked_fill(~mask, float('-inf'))\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)  # (batch_size, seq_len, seq_len)\n",
    "        attention_output = torch.matmul(attention_weights, V)    # (batch_size, seq_len, embedding_dim)\n",
    "        return attention_output\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    \"\"\"Transformer layer with self-attention and feed-forward network.\"\"\"\n",
    "    def __init__(self, embedding_dim, dropout):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.attn = UnidirectionalSelfAttention(embedding_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batch_size, seq_len, embedding_dim]\n",
    "        attn_output = self.attn(x)\n",
    "        x = self.layer_norm(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.layer_norm(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "\n",
    "class SASRec(nn.Module):\n",
    "    \"\"\"\n",
    "    SASRec: Self-Attentive Sequential Recommendation Model.\n",
    "\n",
    "    Architecture:\n",
    "    - Item embeddings + Position embeddings\n",
    "    - Stack of Transformer layers\n",
    "    - Output: logits for next item prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, num_items, embedding_dim=64, max_length=50, num_layers=2, dropout=0.2, std=1e-3):\n",
    "        super(SASRec, self).__init__()\n",
    "        self.std = std\n",
    "        self.item_embedding = nn.Embedding(num_items + 1, embedding_dim, padding_idx=num_items)  # padding\n",
    "        self.position_embedding = nn.Embedding(max_length, embedding_dim)\n",
    "        self.attn_layers = nn.ModuleList([TransformerLayer(embedding_dim, dropout) for _ in range(num_layers)])\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights.\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.std)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.std)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, item_ids):\n",
    "        batch_size, seq_len = item_ids.shape  # [batch_size, seq_len]\n",
    "        positions = torch.arange(seq_len, device=item_ids.device).unsqueeze(0).repeat(batch_size, 1)  # [batch_size, seq_len]\n",
    "        item_embeds = self.item_embedding(item_ids)      # [batch_size, seq_len, embedding_dim]\n",
    "        pos_embeds = self.position_embedding(positions)  # [batch_size, seq_len, embedding_dim]\n",
    "        x = item_embeds + pos_embeds\n",
    "        for attn_layer in self.attn_layers:\n",
    "            x = attn_layer(x)\n",
    "        logits = torch.matmul(x, self.item_embedding.weight[:-1].T)  # [batch_size, seq_len, num_items]\n",
    "        return logits\n",
    "\n",
    "print(\"‚úÖ Model classes loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IezCkq0CR8fB"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--mpLjkpR8fB"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class Metrics:\n",
    "    \"\"\"Metrics calculator for Hit Rate, NDCG, and MRR.\"\"\"\n",
    "    def __init__(self, topk_list):\n",
    "        self.topk_list = topk_list\n",
    "        self.hit_total = {k: 0 for k in self.topk_list}\n",
    "        self.ndcg_total = {k: 0 for k in self.topk_list}\n",
    "        self.mrr_total = {k: 0 for k in self.topk_list}\n",
    "        self.rec_list = []  # Stores (user_index, recommendation_list, target_item)\n",
    "        self.total_nums = 0\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"Get averaged metrics.\"\"\"\n",
    "        hit_total = {k: self.hit_total[k] / self.total_nums for k in self.topk_list}\n",
    "        ndcg_total = {k: self.ndcg_total[k] / self.total_nums for k in self.topk_list}\n",
    "        mrr_total = {k: self.mrr_total[k] / self.total_nums for k in self.topk_list}\n",
    "        return hit_total, ndcg_total, mrr_total, self.rec_list\n",
    "\n",
    "    def accumulate(self, ranks_list, y, start=0):\n",
    "        \"\"\"\n",
    "        Accumulate metrics for a batch.\n",
    "\n",
    "        Args:\n",
    "            ranks_list: List of recommendation lists for each user in batch\n",
    "            y: List of target items\n",
    "            start: Starting user index (for tracking user IDs)\n",
    "        \"\"\"\n",
    "        batch_size = len(y)\n",
    "        for i in range(batch_size):\n",
    "            ranks, true_item = ranks_list[i], y[i]\n",
    "            if true_item in ranks:\n",
    "                rank = ranks.index(true_item) + 1\n",
    "                self.rec_list.append((start+i, ranks, true_item))\n",
    "                for k in self.topk_list:\n",
    "                    if rank <= k:\n",
    "                        self.hit_total[k] += 1\n",
    "                        self.ndcg_total[k] += 1 / math.log2(rank + 1)\n",
    "                        self.mrr_total[k] += 1 / rank\n",
    "        self.total_nums += batch_size\n",
    "\n",
    "\n",
    "def get_top_k_recommendations(scores, all_records, k, phase):\n",
    "    \"\"\"\n",
    "    Get top-k recommendations, filtering out already-interacted items.\n",
    "\n",
    "    Args:\n",
    "        scores: Prediction scores [batch_size, num_items]\n",
    "        all_records: List of user interaction sequences\n",
    "        k: Number of recommendations to return\n",
    "        phase: 'valid' or 'test'\n",
    "\n",
    "    Returns:\n",
    "        Top-k item indices for each user\n",
    "    \"\"\"\n",
    "    delta = 2 if phase == 'valid' else 1\n",
    "    for idx, interacted_items in enumerate(all_records):\n",
    "        # Mask out items user has already interacted with (except last delta items)\n",
    "        scores[idx, interacted_items[:-delta]] = -torch.inf\n",
    "    top_scores, top_indices = torch.topk(scores, k, dim=1)\n",
    "    return top_indices\n",
    "\n",
    "print(\"‚úÖ Utility classes loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INaRghWMR8fB"
   },
   "source": "## Step 5: Training and Evaluation Functions\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "499S_c6HR8fC"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def train(dataloader, model, loss_func, optimizer, epoch, device, verbose=100):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        dataloader: DataLoader for training data\n",
    "        model: SASRec model\n",
    "        loss_func: Loss function\n",
    "        optimizer: Optimizer\n",
    "        epoch: Current epoch number\n",
    "        device: Device to run on\n",
    "        verbose: Print loss every N batches\n",
    "    \"\"\"\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        logits = model(X)                         # [batch_size, seq_len, num_items]\n",
    "        logits = logits.view(-1, logits.size(2))  # [batch_size * seq_len, num_items]\n",
    "        y = y.view(-1)                            # [batch_size * seq_len]\n",
    "        loss = loss_func(logits, y)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if batch % verbose == 0:\n",
    "            print(f\"loss: {train_loss/(batch+1):>7f}  [{batch+1:>5d}/{num_batches:>5d}] epoch: {epoch}\")\n",
    "\n",
    "\n",
    "def test(dataset, model, device, batch_size, topk_list, phase):\n",
    "    \"\"\"\n",
    "    Evaluate the model on validation or test set.\n",
    "\n",
    "    Args:\n",
    "        dataset: ItemSequenceDataset\n",
    "        model: SASRec model\n",
    "        device: Device to run on\n",
    "        batch_size: Batch size for evaluation\n",
    "        topk_list: List of top-k values for evaluation\n",
    "        phase: 'valid' or 'test'\n",
    "\n",
    "    Returns:\n",
    "        hit: Hit Rate dictionary\n",
    "        ndcg: NDCG dictionary\n",
    "        mrr: MRR dictionary\n",
    "        rec_list: List of (user_index, recommendation_list, target_item)\n",
    "    \"\"\"\n",
    "    X_all, y_all = (dataset.X_valid, dataset.y_valid) if phase == 'valid' else (dataset.X_test, dataset.y_test)\n",
    "    model.eval()\n",
    "    metrics = Metrics(topk_list)\n",
    "    with torch.no_grad():\n",
    "        start = 0\n",
    "        while True:\n",
    "            end = start + batch_size\n",
    "            if end > len(y_all):\n",
    "                end = len(y_all)\n",
    "            X = X_all[start:end]\n",
    "            y = y_all[start:end]\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Get prediction scores for the last position in sequence\n",
    "            scores = model(X)[:, -1, :].squeeze(1)  # [batch_size, num_items]\n",
    "            # Get top-k recommendations\n",
    "            ranks_list = get_top_k_recommendations(scores, dataset.all_records[start:end], max(topk_list), phase)\n",
    "            metrics.accumulate(ranks_list.tolist(), y.tolist(), start)\n",
    "            start += batch_size\n",
    "            if end == len(y_all):\n",
    "                break\n",
    "    hit, ndcg, mrr, rec_list = metrics.get()\n",
    "    print(f'[{phase}]')\n",
    "    print(\"Hit:\", hit)\n",
    "    print(\"NDCG:\", ndcg)\n",
    "    print(\"MRR:\", mrr)\n",
    "    return hit, ndcg, mrr, rec_list\n",
    "\n",
    "print(\"‚úÖ Training and evaluation functions loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaoBg6XfR8fC"
   },
   "source": [
    "## Step 6: Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x19TSVuyR8fC"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Loading Dataset\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Load dataset\n",
    "dataset = ItemSequenceDataset(FILEPATH, MAX_LENGTH)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Training samples: {len(dataset)}\")\n",
    "print(f\"Validation samples: {len(dataset.y_valid)}\")\n",
    "print(f\"Test samples: {len(dataset.y_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlVxeJ4eR8fC"
   },
   "source": [
    "## Step 7: Initialize Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQJntRjWR8fC"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Initializing Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create model\n",
    "model = SASRec(\n",
    "    num_items=dataset.num_items,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    max_length=MAX_LENGTH,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel created!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Initialize optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=dataset.num_items)\n",
    "\n",
    "print(f\"Optimizer: Adam (lr={LEARNING_RATE})\")\n",
    "print(f\"Loss function: CrossEntropyLoss\")\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peKCORmeR8fC"
   },
   "source": [
    "## Step 8: Initial Evaluation (Before Training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmbwHbS7R8fC"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Initial Evaluation (Untrained Model)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\nValidation Set:\")\n",
    "hit_valid, ndcg_valid, mrr_valid, _ = test(dataset, model, device, BATCH_SIZE, TOPK_LIST, phase='valid')\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nTest Set:\")\n",
    "hit_test, ndcg_test, mrr_test, _ = test(dataset, model, device, BATCH_SIZE, TOPK_LIST, phase='test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24ypSZO9R8fC"
   },
   "source": [
    "## Step 9: Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgV4VvvtR8fD"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Early stopping patience: {NUM_PATIENCE}\")\n",
    "print(f\"Best model will be saved based on validation NDCG@{max(TOPK_LIST)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "patience = NUM_PATIENCE\n",
    "best_ndcg_valid = 0.0\n",
    "best_valid, best_test, best_epoch = None, None, None\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Train\n",
    "    train(dataloader, model, loss_func, optimizer, epoch, device, VERBOSE)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    hit_valid, ndcg_valid, mrr_valid, rec_list_valid = test(\n",
    "        dataset, model, device, BATCH_SIZE, TOPK_LIST, phase='valid'\n",
    "    )\n",
    "\n",
    "    # Evaluate on test set\n",
    "    hit_test, ndcg_test, mrr_test, rec_list_test = test(\n",
    "        dataset, model, device, BATCH_SIZE, TOPK_LIST, phase='test'\n",
    "    )\n",
    "\n",
    "    # Check if this is the best model\n",
    "    current_ndcg = ndcg_valid[max(TOPK_LIST)]\n",
    "    if current_ndcg >= best_ndcg_valid:\n",
    "        patience = NUM_PATIENCE\n",
    "        best_ndcg_valid = current_ndcg\n",
    "        best_valid = (hit_valid, ndcg_valid, mrr_valid)\n",
    "        best_test = (hit_test, ndcg_test, mrr_test)\n",
    "        best_epoch = epoch\n",
    "\n",
    "        # Save model and recommendation lists\n",
    "        checkpoint_path = f\"SASRec/checkpoint/{DATASET_NAME}.pth\"\n",
    "        torch.save(model, checkpoint_path)\n",
    "        print(f\"\\n‚úÖ Best model saved! (NDCG@{max(TOPK_LIST)}: {best_ndcg_valid:.4f})\")\n",
    "\n",
    "        # Save recommendation lists\n",
    "        with open(f'SASRec/checkpoint/{DATASET_NAME}_rec_list_valid.pkl', 'wb') as f:\n",
    "            pickle.dump(rec_list_valid, f)\n",
    "        with open(f'SASRec/checkpoint/{DATASET_NAME}_rec_list_test.pkl', 'wb') as f:\n",
    "            pickle.dump(rec_list_test, f)\n",
    "        print(f\"‚úÖ Recommendation lists saved!\")\n",
    "    else:\n",
    "        patience -= 1\n",
    "        print(f\"\\n‚ö†Ô∏è  No improvement. Patience: {patience}/{NUM_PATIENCE}\")\n",
    "        if patience == 0:\n",
    "            print(f\"\\nüõë Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training Complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Best epoch: {best_epoch}\")\n",
    "print(f\"Best validation NDCG@{max(TOPK_LIST)}: {best_ndcg_valid:.4f}\")\n",
    "print(f\"\\nBest validation results:\")\n",
    "print(f\"  Hit: {best_valid[0]}\")\n",
    "print(f\"  NDCG: {best_valid[1]}\")\n",
    "print(f\"  MRR: {best_valid[2]}\")\n",
    "print(f\"\\nBest test results:\")\n",
    "print(f\"  Hit: {best_test[0]}\")\n",
    "print(f\"  NDCG: {best_test[1]}\")\n",
    "print(f\"  MRR: {best_test[2]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMhk0q7MR8fD"
   },
   "source": [
    "## Step 10: Save Training Log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nG_zCoqRR8fD"
   },
   "outputs": [],
   "source": [
    "# Save training log\n",
    "log_file = f\"SASRec/checkpoint/{DATASET_NAME}.log\"\n",
    "with open(log_file, 'w') as f:\n",
    "    f.write(f'Dataset: {DATASET_NAME}\\n')\n",
    "    f.write(f'Best epoch: {best_epoch}\\n')\n",
    "    f.write(f'Best validation NDCG@{max(TOPK_LIST)}: {best_ndcg_valid:.4f}\\n')\n",
    "    f.write(f'\\nBest validation results:\\n')\n",
    "    f.write(f'  Hit: {best_valid[0]}\\n')\n",
    "    f.write(f'  NDCG: {best_valid[1]}\\n')\n",
    "    f.write(f'  MRR: {best_valid[2]}\\n')\n",
    "    f.write(f'\\nBest test results:\\n')\n",
    "    f.write(f'  Hit: {best_test[0]}\\n')\n",
    "    f.write(f'  NDCG: {best_test[1]}\\n')\n",
    "    f.write(f'  MRR: {best_test[2]}\\n')\n",
    "\n",
    "print(f\"‚úÖ Training log saved: {log_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HB3HVFH0R8fD"
   },
   "source": [
    "## Step 11: Verify Output Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91Q7uJo_R8fD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Verifying Output Files\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if files exist\n",
    "files_to_check = [\n",
    "    f\"SASRec/checkpoint/{DATASET_NAME}.pth\",\n",
    "    f\"SASRec/checkpoint/{DATASET_NAME}_rec_list_valid.pkl\",\n",
    "    f\"SASRec/checkpoint/{DATASET_NAME}_rec_list_test.pkl\",\n",
    "    f\"SASRec/checkpoint/{DATASET_NAME}.log\"\n",
    "]\n",
    "\n",
    "for file_path in files_to_check:\n",
    "    if os.path.exists(file_path):\n",
    "        size = os.path.getsize(file_path)\n",
    "        print(f\"‚úÖ {file_path} ({size:,} bytes)\")\n",
    "    else:\n",
    "        print(f\"‚ùå {file_path} - NOT FOUND\")\n",
    "\n",
    "# Load and verify recommendation lists\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Verifying Recommendation Lists\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "with open(f'SASRec/checkpoint/{DATASET_NAME}_rec_list_valid.pkl', 'rb') as f:\n",
    "    rec_list_valid = pickle.load(f)\n",
    "with open(f'SASRec/checkpoint/{DATASET_NAME}_rec_list_test.pkl', 'rb') as f:\n",
    "    rec_list_test = pickle.load(f)\n",
    "\n",
    "print(f\"\\nValidation recommendations: {len(rec_list_valid)} entries\")\n",
    "print(f\"Test recommendations: {len(rec_list_test)} entries\")\n",
    "\n",
    "# Show sample\n",
    "if rec_list_valid:\n",
    "    sample = rec_list_valid[0]\n",
    "    print(f\"\\nSample validation entry:\")\n",
    "    print(f\"  User index: {sample[0]}\")\n",
    "    print(f\"  Recommendation list: {sample[1][:5]}... (showing first 5)\")\n",
    "    print(f\"  Target item: {sample[2]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ SASRec Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Use the pickle files in Stage 1 extraction:\")\n",
    "print(\"   - SASRec/checkpoint/Grocery_and_Gourmet_Food_rec_list_valid.pkl\")\n",
    "print(\"   - SASRec/checkpoint/Grocery_and_Gourmet_Food_rec_list_test.pkl\")\n",
    "print(\"2. Run Stage1_Extraction_Colab.ipynb to extract personalized information\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
